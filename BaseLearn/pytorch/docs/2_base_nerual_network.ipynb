{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络包nn和优化器optm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - torch.nn: pytorch专门为神经网络设计的模型化接口\n",
    " - `import torch.nn as nn`\n",
    " - `import torch.nn.functional as F`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义一个网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 继承nn.Module，实现它的forward方法。Pytorch会根据autograd，自动实现backward函数。\n",
    "- forward函数中可使用任何的Tensor支持的函数，还可以使用if、for、print、log等python语法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        # 卷积层 单通道，6输出通道，卷积核为3*3\n",
    "        self.conv1=nn.Conv2d(1,6,3)\n",
    "        # 线性层 输入1350个特征，输出10个特征\n",
    "        self.fc1=nn.Linear(1350,10)\n",
    "    # 正向传播\n",
    "    def forward(self,x):\n",
    "        print(x.size())\n",
    "        # 卷积-> 激活 -> 池化\n",
    "        x=self.conv1(x)\n",
    "        x=F.relu(x)\n",
    "        print(x.size())\n",
    "        x=F.max_pool2d(x,(2,2))\n",
    "        x=F.relu(x)\n",
    "        print(x.size())\n",
    "        # reshape\n",
    "        x=x.view(x.size()[0],-1)\n",
    "        print(x.size())\n",
    "        x=self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=1350, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net=Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.0079,  0.1297, -0.2451],\n",
      "          [ 0.1030,  0.1361,  0.1590],\n",
      "          [ 0.2589,  0.3319,  0.0626]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2521, -0.2298,  0.1167],\n",
      "          [-0.1748,  0.1688, -0.1334],\n",
      "          [-0.2969, -0.0790, -0.0943]]],\n",
      "\n",
      "\n",
      "        [[[-0.2975, -0.0407, -0.0770],\n",
      "          [ 0.3035,  0.0179,  0.2448],\n",
      "          [ 0.1437,  0.2363, -0.0439]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0701, -0.0349, -0.1852],\n",
      "          [-0.2790, -0.0468,  0.2768],\n",
      "          [-0.1921,  0.0059,  0.1197]]],\n",
      "\n",
      "\n",
      "        [[[-0.3148,  0.0139,  0.1341],\n",
      "          [ 0.2395, -0.2048, -0.1377],\n",
      "          [ 0.0726, -0.0313, -0.3294]]],\n",
      "\n",
      "\n",
      "        [[[-0.0025,  0.3097,  0.1006],\n",
      "          [-0.2884,  0.2416, -0.2444],\n",
      "          [-0.1085, -0.0013,  0.1929]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3189, -0.2133, -0.0736,  0.2419,  0.1609, -0.2002],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0041,  0.0105, -0.0018,  ...,  0.0131,  0.0230,  0.0180],\n",
      "        [-0.0057,  0.0131,  0.0198,  ..., -0.0195,  0.0180,  0.0201],\n",
      "        [-0.0016, -0.0019, -0.0038,  ..., -0.0173, -0.0196,  0.0270],\n",
      "        ...,\n",
      "        [-0.0221, -0.0135,  0.0051,  ...,  0.0174,  0.0180,  0.0130],\n",
      "        [-0.0041,  0.0120, -0.0180,  ..., -0.0199,  0.0010, -0.0017],\n",
      "        [ 0.0138, -0.0062,  0.0174,  ...,  0.0129, -0.0230, -0.0266]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0189,  0.0213, -0.0265, -0.0227, -0.0212,  0.0182,  0.0218,  0.0223,\n",
      "        -0.0059, -0.0010], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 网络的可学习参数\n",
    "for parameters in net.parameters():\n",
    "    print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([6, 1, 3, 3])\n",
      "conv1.bias torch.Size([6])\n",
      "fc1.weight torch.Size([10, 1350])\n",
      "fc1.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# 获取返回可学习的参数及名称\n",
    "for name,parameters in net.named_parameters():\n",
    "    print(name,parameters.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- forward()的输入输出都是Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([1, 6, 30, 30])\n",
      "torch.Size([1, 6, 15, 15])\n",
      "torch.Size([1, 1350])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input=torch.randn(1,1,32,32)\n",
    "out=net(input)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 反向传播前，先要将所有参数的梯度清零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.0079,  0.1297, -0.2451],\n",
      "          [ 0.1030,  0.1361,  0.1590],\n",
      "          [ 0.2589,  0.3319,  0.0626]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2521, -0.2298,  0.1167],\n",
      "          [-0.1748,  0.1688, -0.1334],\n",
      "          [-0.2969, -0.0790, -0.0943]]],\n",
      "\n",
      "\n",
      "        [[[-0.2975, -0.0407, -0.0770],\n",
      "          [ 0.3035,  0.0179,  0.2448],\n",
      "          [ 0.1437,  0.2363, -0.0439]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0701, -0.0349, -0.1852],\n",
      "          [-0.2790, -0.0468,  0.2768],\n",
      "          [-0.1921,  0.0059,  0.1197]]],\n",
      "\n",
      "\n",
      "        [[[-0.3148,  0.0139,  0.1341],\n",
      "          [ 0.2395, -0.2048, -0.1377],\n",
      "          [ 0.0726, -0.0313, -0.3294]]],\n",
      "\n",
      "\n",
      "        [[[-0.0025,  0.3097,  0.1006],\n",
      "          [-0.2884,  0.2416, -0.2444],\n",
      "          [-0.1085, -0.0013,  0.1929]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3189, -0.2133, -0.0736,  0.2419,  0.1609, -0.2002],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0041,  0.0105, -0.0018,  ...,  0.0131,  0.0230,  0.0180],\n",
      "        [-0.0057,  0.0131,  0.0198,  ..., -0.0195,  0.0180,  0.0201],\n",
      "        [-0.0016, -0.0019, -0.0038,  ..., -0.0173, -0.0196,  0.0270],\n",
      "        ...,\n",
      "        [-0.0221, -0.0135,  0.0051,  ...,  0.0174,  0.0180,  0.0130],\n",
      "        [-0.0041,  0.0120, -0.0180,  ..., -0.0199,  0.0010, -0.0017],\n",
      "        [ 0.0138, -0.0062,  0.0174,  ...,  0.0129, -0.0230, -0.0266]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0189,  0.0213, -0.0265, -0.0227, -0.0212,  0.0182,  0.0218,  0.0223,\n",
      "        -0.0059, -0.0010], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.ones_like(out))\n",
    "print(net.conv1.weight)\n",
    "print(net.conv1.bias)\n",
    "print(net.fc1.weight)\n",
    "print(net.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.nn只支持mini-batches，不支持一次只输入一个样本，每次需要一个batch。所以上述输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]])\n",
      "tensor(29.7409, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "y=torch.arange(0,10).view(1,10).float()\n",
    "print(y)\n",
    "criterion=nn.MSELoss() # 计算均方误差\n",
    "loss=criterion(out,y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 反向传播计算所有参数的梯度后，需要优化方法更新权重和参数\n",
    "- torch.optim提供大多数的优化方法，例如RMSProp、Adam、SGD等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([1, 6, 30, 30])\n",
      "torch.Size([1, 6, 15, 15])\n",
      "torch.Size([1, 1350])\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0079,  0.1297, -0.2451],\n",
      "          [ 0.1030,  0.1361,  0.1590],\n",
      "          [ 0.2589,  0.3319,  0.0626]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2521, -0.2298,  0.1167],\n",
      "          [-0.1748,  0.1688, -0.1334],\n",
      "          [-0.2969, -0.0790, -0.0943]]],\n",
      "\n",
      "\n",
      "        [[[-0.2975, -0.0407, -0.0770],\n",
      "          [ 0.3035,  0.0179,  0.2448],\n",
      "          [ 0.1437,  0.2363, -0.0439]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0701, -0.0349, -0.1852],\n",
      "          [-0.2790, -0.0468,  0.2768],\n",
      "          [-0.1921,  0.0059,  0.1197]]],\n",
      "\n",
      "\n",
      "        [[[-0.3148,  0.0139,  0.1341],\n",
      "          [ 0.2395, -0.2048, -0.1377],\n",
      "          [ 0.0726, -0.0313, -0.3294]]],\n",
      "\n",
      "\n",
      "        [[[-0.0025,  0.3097,  0.1006],\n",
      "          [-0.2884,  0.2416, -0.2444],\n",
      "          [-0.1085, -0.0013,  0.1929]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3189, -0.2133, -0.0736,  0.2419,  0.1609, -0.2002],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0041,  0.0105, -0.0018,  ...,  0.0131,  0.0230,  0.0180],\n",
      "        [-0.0057,  0.0131,  0.0198,  ..., -0.0195,  0.0180,  0.0201],\n",
      "        [-0.0016, -0.0019, -0.0038,  ..., -0.0173, -0.0196,  0.0270],\n",
      "        ...,\n",
      "        [-0.0221, -0.0135,  0.0051,  ...,  0.0174,  0.0180,  0.0130],\n",
      "        [-0.0041,  0.0120, -0.0180,  ..., -0.0199,  0.0010, -0.0017],\n",
      "        [ 0.0138, -0.0062,  0.0174,  ...,  0.0129, -0.0230, -0.0266]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0189,  0.0213, -0.0265, -0.0227, -0.0212,  0.0182,  0.0218,  0.0223,\n",
      "        -0.0059, -0.0010], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0033,  0.1350, -0.2439],\n",
      "          [ 0.1101,  0.1271,  0.1598],\n",
      "          [ 0.2620,  0.3520,  0.0535]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2498, -0.2177,  0.1196],\n",
      "          [-0.1780,  0.1596, -0.1401],\n",
      "          [-0.2925, -0.0801, -0.1027]]],\n",
      "\n",
      "\n",
      "        [[[-0.2818, -0.0490, -0.0736],\n",
      "          [ 0.2945,  0.0218,  0.2520],\n",
      "          [ 0.1513,  0.2386, -0.0468]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0652, -0.0367, -0.1742],\n",
      "          [-0.2805, -0.0489,  0.2805],\n",
      "          [-0.2023,  0.0028,  0.1043]]],\n",
      "\n",
      "\n",
      "        [[[-0.3008,  0.0065,  0.1382],\n",
      "          [ 0.2426, -0.1976, -0.1250],\n",
      "          [ 0.0780, -0.0272, -0.3181]]],\n",
      "\n",
      "\n",
      "        [[[-0.0171,  0.3160,  0.0962],\n",
      "          [-0.2865,  0.2493, -0.2317],\n",
      "          [-0.1112,  0.0035,  0.1947]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3090, -0.2179, -0.0773,  0.2360,  0.1522, -0.2053],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0041,  0.0105, -0.0018,  ...,  0.0120,  0.0218,  0.0179],\n",
      "        [-0.0057,  0.0131,  0.0198,  ..., -0.0181,  0.0194,  0.0202],\n",
      "        [-0.0016, -0.0019, -0.0038,  ..., -0.0143, -0.0166,  0.0272],\n",
      "        ...,\n",
      "        [-0.0221, -0.0135,  0.0051,  ...,  0.0291,  0.0300,  0.0138],\n",
      "        [-0.0041,  0.0120, -0.0180,  ..., -0.0069,  0.0144, -0.0008],\n",
      "        [ 0.0138, -0.0062,  0.0174,  ...,  0.0264, -0.0091, -0.0256]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0204,  0.0231, -0.0227, -0.0174, -0.0120,  0.0280,  0.0335,  0.0375,\n",
      "         0.0110,  0.0165], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim\n",
    "out=net(input)\n",
    "# 输出初始参数\n",
    "print(net.conv1.weight)\n",
    "print(net.conv1.bias)\n",
    "print(net.fc1.weight)\n",
    "print(net.fc1.bias)\n",
    "# 定义loss计算方法\n",
    "criterion=nn.MSELoss()\n",
    "loss=criterion(out,y)\n",
    "# 定义优化器\n",
    "optimizer=torch.optim.SGD(net.parameters(),lr=0.01)\n",
    "optimizer.zero_grad() # 先梯度清零，与net.zero_grad()效果一致\n",
    "loss.backward()\n",
    "# 更新参数\n",
    "optimizer.step() \n",
    "# 查看更新后的参数\n",
    "print(net.conv1.weight)\n",
    "print(net.conv1.bias)\n",
    "print(net.fc1.weight)\n",
    "print(net.fc1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
